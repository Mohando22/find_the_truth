{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun résultat trouvé pour 'réunissons la France des RER et des TER'.\n",
      "Essayez une phrase plus générale ou différente.\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Clé API Google FactCheck Claim\n",
    "API_KEY = \"AIzaSyB1lz0eog42IEf7oTVfPIt6SSKvC4LC31w\"\n",
    "\n",
    "# URL de l'API Google FactCheck Claim\n",
    "url = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "\n",
    "def verifier_information(query, language=\"fr\"):\n",
    "    \"\"\"\n",
    "    Fonction pour vérifier une information en utilisant l'API Google FactCheck.\n",
    "    Affiche les résultats de fact-checking si disponibles.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'key': API_KEY,\n",
    "        'query': query,\n",
    "        'languageCode': language\n",
    "    }\n",
    "    \n",
    "    # Envoyer la requête GET à l'API\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    # Vérifier le statut de la réponse\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Vérifier s'il y a des affirmations dans les données\n",
    "        if \"claims\" in data and len(data[\"claims\"]) > 0:\n",
    "            print(f\"Résultats pour la requête : '{query}'\")\n",
    "            for claim in data[\"claims\"]:\n",
    "                # Récupérer les informations principales\n",
    "                text = claim.get(\"text\", \"Aucun texte trouvé\")\n",
    "                claimant = claim.get(\"claimant\", \"Source inconnue\")\n",
    "                review = claim.get(\"claimReview\", [{}])[0]\n",
    "                rating = review.get(\"textualRating\", \"Pas d'évaluation\")\n",
    "                review_url = review.get(\"url\", \"URL non disponible\")\n",
    "                \n",
    "                # Afficher les résultats\n",
    "                print(f\"\\nTexte : {text}\")\n",
    "                print(f\"Source : {claimant}\")\n",
    "                print(f\"Évaluation : {rating}\")\n",
    "                print(f\"URL de l'évaluation : {review_url}\")\n",
    "                print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"Aucun résultat trouvé pour '{query}'.\")\n",
    "            print(\"Essayez une phrase plus générale ou différente.\")\n",
    "    else:\n",
    "        print(\"Erreur lors de la requête:\", response.status_code)\n",
    "        print(\"Vérifiez votre clé API ou les paramètres de requête.\")\n",
    "\n",
    "# Exemple d'utilisation avec plusieurs requêtes pour tester la couverture\n",
    "query_list = [\"réunissons la France des RER et des TER\"]\n",
    "for query in query_list:\n",
    "    verifier_information(query)\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: ['politique']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # if used in preprocessing\n",
    "\n",
    "# Load your trained model\n",
    "import joblib\n",
    "model = joblib.load('../model/pipeline_logistic_regression.pkl')\n",
    "\n",
    "# Example text input\n",
    "# Exemple de texte d'entrée\n",
    "text_input = \"\"\"Finances publiques : Michel Barnier incité à mieux taxer les retraités\n",
    "\n",
    "A l’orée du débat sur le budget 2025, le conseil des prélèvements obligatoires formule treize propositions pour rendre plus juste la taxation des revenus, notamment en alourdissant la fiscalité sur les retraités et les riches.\"\"\"\n",
    "\n",
    "# Transformez `text_input` en liste\n",
    "text_input = [text_input]  # Encapsuler dans une liste\n",
    "\n",
    "# Prédiction\n",
    "predictions = model.predict(text_input)\n",
    "\n",
    "# Affichage des prédictions\n",
    "print(\"Predicted class:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DEFAULT_CLIENT_SERVICE_URLS' from 'googletrans.constants' (/home/mohand/.local/lib/python3.10/site-packages/googletrans/constants.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Charger les données depuis le fichier CSV\u001b[39;00m\n\u001b[1;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemonde_articles.csv.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Remplacez par le chemin de votre fichier CSV\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/googletrans/__init__.py:6\u001b[0m\n\u001b[1;32m      2\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.0.0-rc.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LANGCODES, LANGUAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/googletrans/client.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urls, utils\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgtoken\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenAcquirer\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     DEFAULT_CLIENT_SERVICE_URLS,\n\u001b[1;32m     20\u001b[0m     DEFAULT_FALLBACK_SERVICE_URLS,\n\u001b[1;32m     21\u001b[0m     DEFAULT_USER_AGENT, LANGCODES, LANGUAGES, SPECIAL_CASES,\n\u001b[1;32m     22\u001b[0m     DEFAULT_RAISE_EXCEPTION, DUMMY_DATA\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translated, Detected, TranslatedPart\n\u001b[1;32m     26\u001b[0m EXCLUDES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mca\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DEFAULT_CLIENT_SERVICE_URLS' from 'googletrans.constants' (/home/mohand/.local/lib/python3.10/site-packages/googletrans/constants.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from googletrans import Translator\n",
    "\n",
    "# Charger les données depuis le fichier CSV\n",
    "file_path = 'lemonde_articles.csv.csv'  # Remplacez par le chemin de votre fichier CSV\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Initialiser le traducteur\n",
    "translator = Translator()\n",
    "\n",
    "# Fonction pour traduire un texte en anglais\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        translation = translator.translate(text, dest='en')\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur de traduction: {e}\")\n",
    "        return text  # Si la traduction échoue, retourner le texte original\n",
    "\n",
    "# Traduire tous les titres en anglais\n",
    "data['title_translated'] = data['title'].apply(translate_to_english)\n",
    "\n",
    "# Initialiser le modèle Sentence-BERT pour la similarité sémantique\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Fonction pour calculer la similarité sémantique entre deux textes\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Encoder les deux textes en vecteurs denses\n",
    "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculer la similarité cosinus entre les deux vecteurs\n",
    "    similarity = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    \n",
    "    # Retourner le score de similarité (plus il est proche de 1, plus les textes sont similaires)\n",
    "    return similarity.item()\n",
    "\n",
    "# Exemple d'utilisation avec une entrée utilisateur\n",
    "query = \"Votre requête d'information ici\"  # Remplacez par la requête à vérifier\n",
    "\n",
    "# Calculer la similarité entre la requête et chaque titre traduit\n",
    "data['similarity_score'] = data['title_translated'].apply(lambda x: calculate_similarity(x, query))\n",
    "\n",
    "# Fixer un seuil de similarité pour dire si l'information est présente ou non (par ex. 0.7)\n",
    "threshold = 0.7\n",
    "data['contains_similar_information'] = data['similarity_score'].apply(lambda x: x >= threshold)\n",
    "\n",
    "# Afficher les résultats pour vérifier la présence de l'information similaire\n",
    "print(data[['title', 'title_translated', 'similarity_score', 'contains_similar_information']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduction terminée. Fichier traduit sauvegardé sous 'traduction_sortie.csv'\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import csv\n",
    "\n",
    "def traduire_fichier_csv(fichier_csv, langue_source='fr', langue_cible='en', fichier_sortie='traduction_sortie.csv'):\n",
    "    # Ouvrir le fichier CSV d'entrée\n",
    "    with open(fichier_csv, 'r', encoding='utf-8') as fichier_lecture:\n",
    "        lecteur_csv = csv.reader(fichier_lecture)\n",
    "        \n",
    "        # Créer le fichier CSV de sortie\n",
    "        with open(fichier_sortie, 'w', encoding='utf-8', newline='') as fichier_ecriture:\n",
    "            ecrivain_csv = csv.writer(fichier_ecriture)\n",
    "            \n",
    "            # Traduire chaque ligne\n",
    "            for ligne in lecteur_csv:\n",
    "                # Traduire chaque colonne texte de la ligne\n",
    "                ligne_traduite = [GoogleTranslator(source=langue_source, target=langue_cible).translate(texte) for texte in ligne]\n",
    "                # Écrire la ligne traduite dans le fichier de sortie\n",
    "                ecrivain_csv.writerow(ligne_traduite)\n",
    "\n",
    "    print(f\"Traduction terminée. Fichier traduit sauvegardé sous '{fichier_sortie}'\")\n",
    "\n",
    "# Utilisation de la fonction\n",
    "fichier_csv = 'lemonde_articles.csv'\n",
    "traduire_fichier_csv(fichier_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's bring together the France of RER and TER\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input user  \n",
    "input_user = \"\"\"réunissons la France des RER et des TER\"\"\"\n",
    "# traduction de l'inuput du use : traduction = GoogleTranslator(source='fr', target='en').translate(texte_francais)\n",
    "input_translate = GoogleTranslator(source='fr', target='en').translate(input_user)\n",
    "\n",
    "input_translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3680961a7fbc4cc2922cdb7da324a353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d898ac0620324fe49f581978a0f24f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4301e451984a2f9dc25b83303663ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34da46dd47f74dd984c58f978d25d02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31d7e7224e14b7ab97274457bd0dbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases similaires trouvées :\n",
      "- \"For an ecological labourism, let's unite the France of RER and TER\" Published today at 11:00 a.m. 2024-10-15 11:20:55 (Similarité: 8.27)\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import CrossEncoder\n",
    "import csv\n",
    "def traduire_input(input_user, langue_source='fr', langue_cible='en'):\n",
    "    return GoogleTranslator(source=langue_source, target=langue_cible).translate(input_user)\n",
    "\n",
    "# Fonction de recherche par compréhension sémantique avec un Cross-Encoder\n",
    "def rechercher_similarite_par_comprehension(input_user_traduit, fichier_csv_traduit, seuil=0.7):\n",
    "    # Charger le modèle Cross-Encoder\n",
    "    model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "    \n",
    "    # Ouvrir le fichier CSV traduit et stocker les phrases similaires\n",
    "    lignes_similaires = []\n",
    "    with open(fichier_csv_traduit, 'r', encoding='utf-8') as fichier_lecture:\n",
    "        lecteur_csv = csv.reader(fichier_lecture)\n",
    "        \n",
    "        # Comparer chaque ligne avec l'input de l'utilisateur\n",
    "        for ligne in lecteur_csv:\n",
    "            texte_ligne = \" \".join(ligne)  # Joindre les colonnes en une seule phrase si nécessaire\n",
    "            # Le cross-encoder traite les deux phrases ensemble\n",
    "            similarite = model.predict([(input_user_traduit, texte_ligne)])[0]\n",
    "            \n",
    "            # Vérifier si la similarité dépasse le seuil\n",
    "            if similarite >= seuil:\n",
    "                lignes_similaires.append((texte_ligne, similarite))\n",
    "    \n",
    "    # Trier les phrases similaires par ordre de similarité (du plus au moins similaire)\n",
    "    lignes_similaires = sorted(lignes_similaires, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return lignes_similaires\n",
    "\n",
    "# Utilisation des fonctions\n",
    "input_user = \"\"\"réunissons la France des RER et des TER\"\"\"\n",
    "input_traduit = traduire_input(input_user)\n",
    "fichier_csv_traduit = 'traduction_sortie.csv'\n",
    "\n",
    "# Rechercher les phrases similaires\n",
    "resultat_similarite = rechercher_similarite_par_comprehension(input_traduit, fichier_csv_traduit)\n",
    "\n",
    "# Afficher le résultat\n",
    "if resultat_similarite:\n",
    "    print(\"Phrases similaires trouvées :\")\n",
    "    for phrase, score in resultat_similarite:\n",
    "        print(f\"- {phrase} (Similarité: {score:.2f})\")\n",
    "else:\n",
    "    print(\"Aucune phrase similaire n'a été trouvée dans le fichier.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_lemondefr/lemonde_articles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m fichier_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_lemondefr/lemonde_articles.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m fichier_traduit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraduction_sortie.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtraduire_fichier_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfichier_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfichier_traduit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Charger le modèle de classification\u001b[39;00m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/pipeline_logistic_regression.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 13\u001b[0m, in \u001b[0;36mtraduire_fichier_csv\u001b[0;34m(fichier_source, fichier_traduit, langue_source, langue_cible)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraduire_fichier_csv\u001b[39m(fichier_source, fichier_traduit, langue_source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m, langue_cible\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Fonction pour traduire le contenu d'un fichier CSV de la langue source à la langue cible.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfichier_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fichier_lecture, \u001b[38;5;28mopen\u001b[39m(fichier_traduit, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fichier_ecriture:\n\u001b[1;32m     14\u001b[0m         lecteur_csv \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(fichier_lecture)\n\u001b[1;32m     15\u001b[0m         ecrivain_csv \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(fichier_ecriture)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_lemondefr/lemonde_articles.csv'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # if used in preprocessing\n",
    "import requests\n",
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import CrossEncoder\n",
    "import csv\n",
    "\n",
    "def traduire_fichier_csv(fichier_source, fichier_traduit, langue_source='fr', langue_cible='en'):\n",
    "    \"\"\"\n",
    "    Fonction pour traduire le contenu d'un fichier CSV de la langue source à la langue cible.\n",
    "    \"\"\"\n",
    "    with open(fichier_source, 'r', encoding='utf-8') as fichier_lecture, open(fichier_traduit, 'w', encoding='utf-8', newline='') as fichier_ecriture:\n",
    "        lecteur_csv = csv.reader(fichier_lecture)\n",
    "        ecrivain_csv = csv.writer(fichier_ecriture)\n",
    "        \n",
    "        for ligne in lecteur_csv:\n",
    "            ligne_traduite = [GoogleTranslator(source=langue_source, target=langue_cible).translate(cell) for cell in ligne]\n",
    "            ecrivain_csv.writerow(ligne_traduite)\n",
    "\n",
    "fichier_source = 'data_lemondefr/lemonde_articles.csv'\n",
    "fichier_traduit = 'traduction_sortie.csv'\n",
    "traduire_fichier_csv(fichier_source, fichier_traduit)\n",
    "# Charger le modèle de classification\n",
    "model = joblib.load('model/pipeline_logistic_regression.pkl')\n",
    "\n",
    "# Définir la clé API pour le fact-checking\n",
    "API_KEY = \"AIzaSyB1lz0eog42IEf7oTVfPIt6SSKvC4LC31w\"\n",
    "url_factcheck = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "\n",
    "def pipeline_text_processing(text_input, fichier_csv_traduit='traduction_sortie.csv', seuil=0.7, langue_source='fr', langue_cible='en'):\n",
    "    \"\"\"\n",
    "    Pipeline de traitement de texte pour la classification et recherche par similarité ou vérification de fact-check.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encapsuler l'input en liste et classer avec le modèle de classification\n",
    "    text_input_list = [text_input]\n",
    "    predicted_class = model.predict(text_input_list)[0]\n",
    "    \n",
    "    # Si l'input est classifié comme politique\n",
    "    if predicted_class == \"politique\":\n",
    "        print(\"Texte classifié comme 'politique'. Recherche de phrases similaires.\")\n",
    "        \n",
    "        # Traduire l'input\n",
    "        input_traduit = GoogleTranslator(source=langue_source, target=langue_cible).translate(text_input)\n",
    "        \n",
    "        # Initialiser le modèle Cross-Encoder pour la similarité\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "        \n",
    "        # Recherche de similarité par compréhension sémantique\n",
    "        lignes_similaires = []\n",
    "        with open(fichier_csv_traduit, 'r', encoding='utf-8') as fichier_lecture:\n",
    "            lecteur_csv = csv.reader(fichier_lecture)\n",
    "            for ligne in lecteur_csv:\n",
    "                texte_ligne = \" \".join(ligne)\n",
    "                similarite = cross_encoder.predict([(input_traduit, texte_ligne)])[0]\n",
    "                if similarite >= seuil:\n",
    "                    lignes_similaires.append((texte_ligne, similarite))\n",
    "        \n",
    "        # Trier et afficher les phrases similaires\n",
    "        lignes_similaires = sorted(lignes_similaires, key=lambda x: x[1], reverse=True)\n",
    "        if lignes_similaires:\n",
    "            print(\"Phrases similaires trouvées :\")\n",
    "            for phrase, score in lignes_similaires:\n",
    "                print(f\"- {phrase} (Similarité: {score:.2f})\")\n",
    "        else:\n",
    "            print(\"Aucune phrase similaire n'a été trouvée dans le fichier.\")\n",
    "    \n",
    "    # Si l'input n'est pas classifié comme politique\n",
    "    else:\n",
    "        print(\"Texte non classifié comme 'politique'. Vérification des informations.\")\n",
    "        \n",
    "        # Paramètres pour l'API de fact-checking\n",
    "        params = {\n",
    "            'key': API_KEY,\n",
    "            'query': text_input,\n",
    "            'languageCode': langue_source\n",
    "        }\n",
    "        \n",
    "        # Requête GET vers l'API Google FactCheck\n",
    "        response = requests.get(url_factcheck, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"claims\" in data and len(data[\"claims\"]) > 0:\n",
    "                print(f\"Résultats de fact-check pour : '{text_input}'\")\n",
    "                for claim in data[\"claims\"]:\n",
    "                    text = claim.get(\"text\", \"Aucun texte trouvé\")\n",
    "                    claimant = claim.get(\"claimant\", \"Source inconnue\")\n",
    "                    review = claim.get(\"claimReview\", [{}])[0]\n",
    "                    rating = review.get(\"textualRating\", \"Pas d'évaluation\")\n",
    "                    review_url = review.get(\"url\", \"URL non disponible\")\n",
    "                    print(f\"\\nTexte : {text}\")\n",
    "                    print(f\"Source : {claimant}\")\n",
    "                    print(f\"Évaluation : {rating}\")\n",
    "                    print(f\"URL de l'évaluation : {review_url}\")\n",
    "                    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            else:\n",
    "                print(f\"Aucun résultat de fact-check trouvé pour '{text_input}'.\")\n",
    "        else:\n",
    "            print(\"Erreur lors de la requête de fact-check:\", response.status_code)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "text_input = \"\"\"En direct, gouvernement Barnier \"\"\"\n",
    "pipeline_text_processing(text_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
